# -*- coding: utf-8 -*-
"""Heart Attack Disease Prediction System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKHmJSpmjCvnmtp7D32KlKXEzlI2xHCh

Heart Disease Prediction System

This project aims to leverage the Heart Attack Risk Prediction Dataset to develop models capable of predicting the likelihood of a heart attack based on various health attributes. The dataset encompasses diverse features such as age, cholesterol levels, blood pressure, smoking habits, exercise patterns, and dietary preferences. The goal is to contribute to proactive strategies for heart disease prevention and management.

1- DATA COLLECTION

Acquired a comprehensive dataset containing information on individuals' health attributes, serving as the foundation for model development.
"""

import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('/content/heart_attack_prediction_dataset.csv')

data.head()

#Displays Number of Rows & Columns.
print("(Rows, columns): " + str(data.shape))

#Displays the Column names
data.columns

#Displays the datatypes
data.dtypes

#Dropping coloumns which are not necessary to increase accuracy
data = data.drop(["Patient ID", "Country", "Continent", "Hemisphere", "Sedentary Hours Per Day", "Income"], axis=1)

#Fetches the number of unique values for each variable.
data.nunique(axis=0)

#Describes the data based on mean, standard deviation, min, max etc
data.describe()

"""2- DATA PREPROCESSING

Conducted thorough preprocessing tasks, including handling missing values, encoding categorical variables, and scaling numerical features to ensure data quality and compatibility with machine learning models.
"""

#Command to find missing values in the data
print(data.isna().sum())

#Count of values in target column
data['Heart Attack Risk'].value_counts()

"""Here, 0- No risk of attack
       1- Risk of Attack
"""

#Encoding to convert categorical values ot numerical values
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

for column in data.columns:
    data["Sex"] = label_encoder.fit_transform(data["Sex"])
    data["Diet"] = label_encoder.fit_transform(data["Diet"])
    data["Blood Pressure"] = label_encoder.fit_transform(data["Blood Pressure"])
    data["Cholesterol"] = label_encoder.fit_transform(data["Cholesterol"])

# Display the data after label encoding
print("\nData after Label Encoding:\n", data)

#Scaling the data to bring in one range as a part of normalization
from sklearn.preprocessing import StandardScaler

scaler_standard = StandardScaler()
data_standard_scaled = scaler_standard.fit_transform(data)

#X- Feature columns; y- target column
#Split the data set into the Training set and Test set
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)

"""3- FEATURE SELECTION

Employed feature selection techniques to identify the most relevant attributes contributing to the prediction of heart attacks, enhancing model efficiency.
"""

from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# Converting X to a DataFrame to keep track of column names
X_df = pd.DataFrame(X, columns=data.columns[:-1])
y_df = pd.DataFrame(y, columns=['target_variable'])

# Instantiating a RandomForestClassifier
model = RandomForestClassifier()

# Number of features you want to keep
num_features_to_keep = 4

# Initialize RFE
rfe = RFE(model, n_features_to_select=num_features_to_keep)

# Fit RFE on data
X_rfe = rfe.fit_transform(X_df, y_df.values.ravel())  # Use .values.ravel() to convert y_df to a 1D array

# Fetching selected features directly from RFE
selected_features = X_df.columns[rfe.support_]

# Display the selected features
print("Selected Features:")
print(selected_features.tolist())

selected_features = X_df.columns[rfe.support_]

# Bar chart to visualize the importance of selected features
plt.figure(figsize=(10, 6))
selected_feature_importances = rfe.estimator_.feature_importances_
plt.bar(range(len(selected_features)), selected_feature_importances, tick_label=selected_features)
plt.title('Feature Importance of Selected Features')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.show()

"""4- MODEL DEVELOPMENT

Implemented Support Vector Machine (SVM), Logistic Regression, and Random Forest models to predict the likelihood of heart attacks based on the selected features.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Training the model
rf_model.fit(x_train, y_train)

# Predictions on the test set
y_pred_rf = rf_model.predict(x_test)

# Evaluating the model performance
accuracy = accuracy_score(y_test, y_pred_rf)
print(f'Model Accuracy: {accuracy * 100:.2f}%')

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

logistic_regression_model = LogisticRegression(random_state=42)

# Training the model
logistic_regression_model.fit(x_train, y_train)

# Making predictions on the test set
y_pred_lr = logistic_regression_model.predict(x_test)

# Evaluating the model performance
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f'Logistic Regression Model Accuracy: {accuracy_lr * 100:.2f}%')

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

svm_model = SVC(random_state=42)

# Training the model
svm_model.fit(x_train, y_train)

# Making predictions on the test set
y_pred_svm = svm_model.predict(x_test)

# Evaluating the model performance
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f'SVM Model Accuracy: {accuracy_svm * 100:.2f}%')

"""5- CROSS VALIDATION

Utilized cross-validation techniques to assess the models' performance, ensuring robustness and generalizability.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

logistic_regression_model = LogisticRegression(random_state=42, max_iter=1000)

# Performing cross-validation
cv_scores_lr = cross_val_score(logistic_regression_model, X, y, cv=5)  # Adjust 'X' and 'y' accordingly

# Printing cross-validation scores
print("Logistic Regression Cross-Validation Scores:", cv_scores_lr)
print("Mean Accuracy: {:.2f}%".format(cv_scores_lr.mean() * 100))

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

svm_model = SVC(random_state=42, max_iter=1000)

# Performing cross-validation
cv_scores_svm = cross_val_score(svm_model, X, y, cv=5)  # Adjust 'X' and 'y' accordingly

# Printing cross-validation scores
print("SVM Cross-Validation Scores:", cv_scores_svm)
print("Mean Accuracy: {:.2f}%".format(cv_scores_svm.mean() * 100))

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

logistic_regression_model = LogisticRegression(random_state=42, max_iter= 1000)

# Performing cross-validation
cv_scores_lr = cross_val_score(logistic_regression_model, X, y, cv=5)  # Adjust 'X' and 'y' accordingly

# Printing cross-validation scores
print("Logistic Regression Cross-Validation Scores:", cv_scores_lr)
print("Mean Accuracy: {:.2f}%".format(cv_scores_lr.mean() * 100))

"""6- HYPER PARAMETER TUNING

Fine-tuned model hyperparameters to optimize predictive accuracy and achieve better performance.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Logistic Regression hyperparameter tuning
param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
logistic_regression_model = LogisticRegression(random_state=42, max_iter= 1000)
grid_search_lr = GridSearchCV(logistic_regression_model, param_grid_lr, cv=5)
grid_search_lr.fit(x_train, y_train)
best_lr_model = grid_search_lr.best_estimator_

# Random Forest hyperparameter tuning
param_grid_rf = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
random_forest_model = RandomForestClassifier(random_state=42 )
grid_search_rf = GridSearchCV(random_forest_model, param_grid_rf, cv=5)
grid_search_rf.fit(x_train, y_train)
best_rf_model = grid_search_rf.best_estimator_

# SVM hyperparameter tuning
param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
svm_model = SVC(random_state=42, max_iter= 1000)
grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=5)
grid_search_svm.fit(x_train, y_train)
best_svm_model = grid_search_svm.best_estimator_

# Evaluating best models on the test set
y_pred_lr = best_lr_model.predict(x_test)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f'Best Logistic Regression Model Accuracy: {accuracy_lr * 100:.2f}%')

y_pred_rf = best_rf_model.predict(x_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f'Best Random Forest Model Accuracy: {accuracy_rf * 100:.2f}%')

y_pred_svm = best_svm_model.predict(x_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f'Best SVM Model Accuracy: {accuracy_svm * 100:.2f}%')

"""7- VALIDATION AND TESTING

Evaluated model performance on validation and testing datasets to measure accuracy and understand each model's predictive capabilities.
"""

from sklearn.metrics import classification_report
def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    report = classification_report(y_true, y_pred)
    print(f"------ {model_name} Model ------")
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:\n", report)

# Evaluating Validation Set
evaluate_model(y_test, y_pred_svm, "SVM")
evaluate_model(y_test, y_pred_lr, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest")

